{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summative 2 Assignment\n",
    "\n",
    "## Summary Statement\n",
    "The following notebook charts the implementation behind a work project, looking at using company financial variables to try to predict subsequent company liquidations. It maps this process to the data science lifecycle, and outlines proposed methods for data discovery and analysis. Different modelling choices are discussed, and a proposal made based on this rhetoric. Plans on how to train, evaluate and deploy this model are discussed, as well the specific reasoning behind the proposed choices for this business case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ConfusionMatrixDisplay'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c09c1d0863b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'matplotlib'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m \u001b[1;31m#metrics\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mStandardScaler\u001b[0m \u001b[1;31m#for scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m \u001b[1;31m#for sperating train and test samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'ConfusionMatrixDisplay'"
     ]
    }
   ],
   "source": [
    "# First import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.naive_bayes import GaussianNB #Gaussian Naive Bayes classifier\n",
    "from sklearn.neighbors import KNeighborsClassifier #Classifier implementing the k-nearest neighbors vote\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz # Decision Tree Classifier using CART 4.5\n",
    "from sklearn import tree # for plotting trees\n",
    "from sklearn.linear_model import LogisticRegression #Logistic Regression Classifier\n",
    "from sklearn.model_selection import GridSearchCV # gridsearch wrapper for hyperparameters\n",
    "from sklearn.preprocessing import LabelEncoder # encoder for mapping categorical string variables to numeric\n",
    "import seaborn as sns #for nice visualisations\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay #metrics\n",
    "from sklearn.preprocessing import StandardScaler #for scaling\n",
    "from sklearn.model_selection import train_test_split #for sperating train and test samples\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Business use case:\n",
    "The ‘Corporate’ division of the team I work in, are interested in seeing if we can create a model which will take company financial information for UK mid-size companies, and use it to predict if that company is likely to enter into an insolvency stage in the coming year. The primary interest is in ensuring that any company which will become insolvent in the subsequent year is positively identified. As a secondary objective, they would also like to limit the number of companies incorrectly being identified as ‘at risk.’\n",
    "\n",
    "Of signifcant importance are:\n",
    "1. Ensuring the above objetcives are validated by appropriate evaluation metrics\n",
    "2. That the choice of model is easily interpretable and explainable to further stakeholders\n",
    "3. That the evaluation of the model is as robust as possible, including maintaining the fidelity of any testing data, and methodology to minimise the risk of overfitting\n",
    "\n",
    "The team has specifically requested that there is no work down for removing outliers. The deliverable should be a pre-trained model which can take a dataframe of financial information for various companies as input, and output a prediction for each company of whether or not that company enters an insolvency stage in the following year.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding:\n",
    "Appropriate data for this task exists on the cloud server across two separate tables. For the purposes of this notebook, I have egressed this, and made it available as 2 csv files. Below, I read in the data as pandas DataFrames. Initial visual inspection of the data confirms it should be appropriate, though some exploration and further cleaning, will be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in Both Files as Tables\n",
    "bvd_df = pd.read_csv('bvd_df.csv')\n",
    "codes_df = pd.read_csv('codes_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bvd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e95799969718>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Inspect top portion of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbvd_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'bvd_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Inspect top portion of the data\n",
    "bvd_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codes_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-ad3b0f084fee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcodes_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'codes_df' is not defined"
     ]
    }
   ],
   "source": [
    "codes_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Munging/Exploratory Analysis\n",
    "### a)\tSummary Exploration\n",
    "The process of cleaning the data, and exploring it go hand in hand. Below, we begin by acquiring summary information for the data. Next we produce a single merged table from the individual data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bvd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-e91bf80d24ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Shape of the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The BvD table has {bvd_df.shape[0]} rows and {bvd_df.shape[1]} columns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'The Codes table has {codes_df.shape[0]} rows and {codes_df.shape[1]} columns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bvd_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Shape of the data\n",
    "print(f'The BvD table has {bvd_df.shape[0]} rows and {bvd_df.shape[1]} columns')\n",
    "print(f'The Codes table has {codes_df.shape[0]} rows and {codes_df.shape[1]} columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bvd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d08d9fec3f38>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Some summary information, include dtypes and count of null values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbvd_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcodes_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bvd_df' is not defined"
     ]
    }
   ],
   "source": [
    "#Some summary information, include dtypes and count of null values\n",
    "bvd_df.info()\n",
    "codes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bvd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-cb2232c51f9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Informtation on distribution of table variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbvd_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mcodes_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bvd_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Informtation on distribution of table variables\n",
    "bvd_df.describe()\n",
    "codes_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from this, that the tables have a sufficient number of records for training a model. Features which are more poorly populated ie those with more null values, include 'number_ccjs', and 'long_term_debt'. The potential features also appear to have significantly varying scales. A final note: sic_division, and incorporation date are not numerical - we will consider this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codes_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-0cd02acfd542>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# This is our outcome variable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mcodes_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodes_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#each entry is only of benefit if all data is present, so we drop NAs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mcodes_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m \u001b[1;31m#simplify the notice codes into a y label\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcodes_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'ch_id_matched'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'ch_id'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#rename the key, for easier merging\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'codes_df' is not defined"
     ]
    }
   ],
   "source": [
    "#We don't need the details of the insolvency code, only the fact that an insolvency notice was posted\n",
    "# We thus start by making a simplified codes table with appropriate labels- which we call 'y'\n",
    "# This is our outcome variable\n",
    "\n",
    "codes_df = codes_df.dropna() #each entry is only of benefit if all data is present, so we drop NAs\n",
    "codes_df['y'] = True #simplify the notice codes into a y label\n",
    "codes_df.rename(columns = {'ch_id_matched':'ch_id'}, inplace = True) #rename the key, for easier merging\n",
    "join_df = codes_df[['ch_id','y']] # select what we need for the merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bvd_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5e7bb5546eb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#merge tables on ch_id\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbvd_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmerge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'left'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'ch_id'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Where there is no insolvency notice present, y will now be a NA value. We need to change this to 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bvd_df' is not defined"
     ]
    }
   ],
   "source": [
    "#merge tables on ch_id\n",
    "df = bvd_df.merge(join_df, how = 'left', on = 'ch_id')\n",
    "#Where there is no insolvency notice present, y will now be a NA value. We need to change this to 0\n",
    "df['y'].fillna(False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4a676c3d8c05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b)\tVisualisation\n",
    "John W. Tukey wrote, “the greatest value of a picture is when it forces us to notice what we never expected to see.”\n",
    "\n",
    "We thus begin the process of visualising the data. This can:\n",
    "1. Highlight any issues with the data. \n",
    "2. We can observe the correlation between independent variables\n",
    "3. We can see how the different labels are separated by these variables\n",
    "4. This information feeds into our decisions for applying data science methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-fc9d6deeb25a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#We plot Correlations between features, using a monotonic relationship\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'spearman'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#We plot Correlations between features, using a monotonic relationship\n",
    "sns.heatmap(df.corr(method ='spearman'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-137374df283e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#and a linear relationship\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;34m'pearson'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#and a linear relationship\n",
    "sns.heatmap(df.corr(method ='pearson'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most features have quite low linear correlation with one another. Exceptions are total assets, current assets, and current liabilities, all being correlated, as well as long term debt and long term liabilities. Those findings all make intuitive sense.\n",
    "When comparing the spearman correlation, we can see that the monotonic correlation is generally higher across the board, with the exception of ccjs. This lends credence to the idea that some of the features that confer explanatory power, do so on differing scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-928aad3d3be4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#We can use boxplots, divided by the target label, to see how well the individual independant variables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#explain the difference in the target label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mboxplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshowfliers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#We can use boxplots, divided by the target label, to see how well the individual independant variables \n",
    "#explain the difference in the target label\n",
    "box = df.boxplot(by='y', showfliers = False, figsize=(15,  15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above boxplot suggests current assets and total assets are particularly good variables to explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'histplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-45b1ae6721ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#current assets, with percentage balanced by class\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m current_assets = sns.histplot(df.dropna(), x = 'current_assets', hue='y', stat = 'percent',\n\u001b[0m\u001b[0;32m      5\u001b[0m                               common_norm = False, bins = 50, log_scale=(True, False))\n\u001b[0;32m      6\u001b[0m \u001b[0mcurrent_assets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'histplot'"
     ]
    }
   ],
   "source": [
    "#A closer look at the distribution of current assets and total assets, seperated by class label\n",
    "#current assets, with percentage balanced by class\n",
    "sns.set(rc={'figure.figsize':(12,6)})\n",
    "current_assets = sns.histplot(df.dropna(), x = 'current_assets', hue='y', stat = 'percent',\n",
    "                              common_norm = False, bins = 50, log_scale=(True, False))\n",
    "current_assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'seaborn' has no attribute 'histplot'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-e480966f7c7b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#total_assets. Just for comparison, I have left this unbalanced\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m total_assets = sns.histplot(df.dropna(), x = 'total_assets', hue='y', stat = 'percent', \n\u001b[0m\u001b[0;32m      4\u001b[0m                             common_norm = True, bins = 100, log_scale=(True, False))\n\u001b[0;32m      5\u001b[0m \u001b[0mtotal_assets\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'seaborn' has no attribute 'histplot'"
     ]
    }
   ],
   "source": [
    "#total_assets. Just for comparison, I have left this unbalanced\n",
    "sns.set(rc={'figure.figsize':(12,6)})\n",
    "total_assets = sns.histplot(df.dropna(), x = 'total_assets', hue='y', stat = 'percent', \n",
    "                            common_norm = True, bins = 100, log_scale=(True, False))\n",
    "total_assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary of the above: it appears asset variables are the best at separating the data by class, but the distribution is log-normal and skewed, and there is still significant overlap of the distributions between classes, so the individual explanatory power is sub-optimal. This is an example of where simple methodology is unlikely to produce a good final model – an opportunity exists to get better results with a machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-a3dd95106a8b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# We can use paired feature plots, to produce a similar analysis to above, but instead splitting the labels across a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# pair of features rather than just one.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mpair\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpairplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Scatter plots of paired Features, with Colour based on Outcome Label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpair\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# We can use paired feature plots, to produce a similar analysis to above, but instead splitting the labels across a \n",
    "# pair of features rather than just one.\n",
    "pair = sns.pairplot(df, hue='y')\n",
    "pair.fig.suptitle('Scatter plots of paired Features, with Colour based on Outcome Label', y=1)\n",
    "pair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see, plotting a feature space with turnover and long term debt, shows promising separability between classes. In general, we still observe a very mixed set of observations, without clear divides/linear seperability.\n",
    "\n",
    "Next we look at missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-0e7884088ea7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'figure.figsize'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m12\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrotation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m60\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'right'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'number of missing values'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAswAAAFkCAYAAAA0dm0EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFqhJREFUeJzt3X9o3PX9wPFXv7kmzuQ6W5eJgimKBBwY2lT/2LQVKWUyZIgY22ZmDIeoTGWd27JVVtNqO1ts3A+rzKmlpGqbzj/EKcgE12C1qHGlVK3DCpWtg8WlutxpkzX3+f7xxeObVd+tZ3qpvcfjL+/eF+714VX1mbsPOi3LsiwAAIBP9D9TPQAAAJzIBDMAACQIZgAASBDMAACQIJgBACBBMAMAQEJuqgdIGRoameoRasrMmafGwYMfTvUYHGf2fPKz49pgz7XBnqunuTn/qWc+YaYsl6ub6hGoAns++dlxbbDn2mDPJwbBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAQkXBXCqVYsWKFbF48eLo6uqK/fv3Tzjv7++Pq666Kq655pp4/vnnJ5y98sorcemll1Y+MQAAVFGukh967rnnYmxsLLZu3Rq7du2Ku+++Ox544IGIiBgaGoq+vr544oknYnR0NDo7O+Piiy+O+vr6+Mc//hGPPPJIHD58eFIvAgAAjpeKPmEeHByM+fPnR0TEnDlzYs+ePeWz3bt3x9y5c6O+vj7y+Xy0tLTE3r17Y3R0NO64447o6emZlMEBAKAaKvqEuVAoRFNTU/lxXV1dHD58OHK5XBQKhcjn8+WzxsbGKBQKsWrVqrjuuuvijDPOOOb3mTnz1Mjl6ioZkQo1N+eP/iK+8Oz55GfHtcGea4M9T72KgrmpqSmKxWL5calUilwu94lnxWIxpk+fHq+++mq8++67sWHDhvjggw9i2bJlce+99ybf5+DBDysZjwo1N+djaGhkqsfgOLPnk58d1wZ7rg32XD2pX0wqCub29vZ4/vnn41vf+lbs2rUrWltby2dtbW3xq1/9KkZHR2NsbCz27dsXbW1t8eyzz5Zfc/HFFx81lgEA4ERQUTAvWrQoduzYEUuWLIksy2LNmjWxcePGaGlpiYULF0ZXV1d0dnZGlmWxbNmyaGhomOy5AQCgKqZlWZZN9RCfxlcQ1eVrn9pgzyc/O64N9lwb7Ll6Urdk+B+XAABAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAm5Sn6oVCpFT09PvPXWW1FfXx933XVXzJ49u3ze398fW7ZsiVwuFzfddFNcdtllceDAgVi+fHmMj49HlmWxatWqOPfccyftQgAA4Hio6BPm5557LsbGxmLr1q1x2223xd13310+Gxoair6+vtiyZUs8/PDD0dvbG2NjY/HrX/86rr322ujr64sbbrghent7J+0iAADgeKnoE+bBwcGYP39+RETMmTMn9uzZUz7bvXt3zJ07N+rr66O+vj5aWlpi79690d3dHfl8PiIixsfHo6GhYRLGBwCA46uiYC4UCtHU1FR+XFdXF4cPH45cLheFQqEcxhERjY2NUSgUYtasWRER8c4778TatWtjw4YNR32fmTNPjVyurpIRqVBzc/7oL+ILz55PfnZcG+y5Ntjz1KsomJuamqJYLJYfl0qlyOVyn3hWLBbLAb1z585YuXJlrFu37pjuXz548MNKxqNCzc35GBoameoxOM7s+eRnx7XBnmuDPVdP6heTiu5hbm9vj4GBgYiI2LVrV7S2tpbP2traYnBwMEZHR2NkZCT27dsXra2tsXPnzli9enU89NBDccEFF1TytgAAUHUVfcK8aNGi2LFjRyxZsiSyLIs1a9bExo0bo6WlJRYuXBhdXV3R2dkZWZbFsmXLoqGhIdasWRP/+c9/4mc/+1lERJxzzjmxatWqSb0YAACYbNOyLMumeohP4yuI6vK1T22w55OfHdcGe64N9lw9k35LBgAA1ArBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACTkKv3BUqkUPT098dZbb0V9fX3cddddMXv27PJ5f39/bNmyJXK5XNx0001x2WWXxfDwcPz4xz+OQ4cOxVe/+tX45S9/GV/60pcm5UIAAOB4qPgT5ueeey7GxsZi69atcdttt8Xdd99dPhsaGoq+vr7YsmVLPPzww9Hb2xtjY2Nx//33xxVXXBGPPfZYfO1rX4utW7dOykUAAMDxUnEwDw4Oxvz58yMiYs6cObFnz57y2e7du2Pu3LlRX18f+Xw+WlpaYu/evRN+ZsGCBfHiiy9+zvEBAOD4qviWjEKhEE1NTeXHdXV1cfjw4cjlclEoFCKfz5fPGhsbo1AoTHi+sbExRkZGku8xc+apkcvVVToiFWhuzh/9RXzh2fPJz45rgz3XBnueehUHc1NTUxSLxfLjUqkUuVzuE8+KxWLk8/ny86ecckoUi8WYMWNG8j0OHvyw0vGoQHNzPoaG0r/E8MVnzyc/O64N9lwb7Ll6Ur+YVHxLRnt7ewwMDERExK5du6K1tbV81tbWFoODgzE6OhojIyOxb9++aG1tjfb29ti+fXtERAwMDMS8efMqfXsAAKiKij9hXrRoUezYsSOWLFkSWZbFmjVrYuPGjdHS0hILFy6Mrq6u6OzsjCzLYtmyZdHQ0BA33XRTdHd3R39/f8ycOTPWr18/mdcCAACTblqWZdlUD/FpfAVRXb72qQ32fPKz49pgz7XBnqvnuNySAQAAtUAwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAICEXCU/dOjQofjJT34S//rXv6KxsTHWrl0bs2bNmvCa++67L/785z9HLpeL5cuXR1tbW7z55ptx5513Rl1dXdTX18fatWvjK1/5yqRcCAAAHA8VfcL8+OOPR2trazz22GNx5ZVXxv333z/h/PXXX4+XX345tm3bFr29vbFy5cqIiFi9enX84he/iL6+vli0aFH8/ve///xXAAAAx1FFwTw4OBjz58+PiIgFCxbESy+9dMT5JZdcEtOmTYuzzjorxsfHY3h4OHp7e+P888+PiIjx8fFoaGj4nOMDAMDxddRbMrZt2xabNm2a8Nzpp58e+Xw+IiIaGxtjZGRkwnmhUIjTTjut/Pjj18yePTsiIl577bXYvHlzPProo8n3njnz1Mjl6o7tSpgUzc35qR6BKrDnk58d1wZ7rg32PPWOGswdHR3R0dEx4bmbb745isViREQUi8WYMWPGhPOmpqby+cev+Tiwn3nmmXjggQfiwQcfPOK+5/928OCHx3YVTIrm5nwMDY0c/YV8odnzyc+Oa4M91wZ7rp7ULyYV3ZLR3t4e27dvj4iIgYGBmDdv3hHnL7zwQpRKpThw4ECUSqWYNWtWPPnkk7F58+bo6+uLs88+u5K3BgCAqqrov5KxdOnS6O7ujqVLl8b06dNj/fr1ERGxbt26uPzyy6OtrS0uvPDCWLx4cZRKpVixYkWMj4/H6tWr48wzz4xbbrklIiIuuuiiuPXWWyfvagAAYJJNy7Ism+ohPo2vIKrL1z61wZ5PfnZcG+y5Nthz9Uz6LRkAAFArBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASKgomA8dOhS33HJLdHZ2xvXXXx/Dw8NHvOa+++6Lq6++OpYsWRK7d++ecPbUU0/F4sWLK5sYAACqqKJgfvzxx6O1tTUee+yxuPLKK+P++++fcP7666/Hyy+/HNu2bYve3t5YuXJl+ezNN9+MP/zhD5Fl2eebHAAAqqCiYB4cHIz58+dHRMSCBQvipZdeOuL8kksuiWnTpsVZZ50V4+PjMTw8HAcPHox77rknli9f/vknBwCAKsgd7QXbtm2LTZs2TXju9NNPj3w+HxERjY2NMTIyMuG8UCjEaaedVn7c2NgY77//fjmWGxoajmm4mTNPjVyu7phey+Robs5P9QhUgT2f/Oy4NthzbbDnqXfUYO7o6IiOjo4Jz918881RLBYjIqJYLMaMGTMmnDc1NZXPP35NoVCI/fv3R09PT4yOjsbbb78dq1evjttvv/1T3/vgwQ8/08Xw+TQ352NoaOToL+QLzZ5PfnZcG+y5Nthz9aR+Manoloz29vbYvn17REQMDAzEvHnzjjh/4YUXolQqxYEDB6JUKkVbW1s8/fTT0dfXF729vXHeeeclYxkAAE4ER/2E+ZMsXbo0uru7Y+nSpTF9+vRYv359RESsW7cuLr/88mhra4sLL7wwFi9eHKVSKVasWDGpQwMAQLVMy07g/1yFryCqy9c+tcGeT352XBvsuTbYc/VM+i0ZAABQKwQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJghkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmAGAIAEwQwAAAmCGQAAEgQzAAAkCGYAAEgQzAAAkCCYAQAgQTADAECCYAYAgATBDAAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCABMEMAAAJ07Isy6Z6CAAAOFH5hBkAABIEMwAAJAhmAABIEMwAAJAgmAEAIEEwAwBAgmCuMYcOHYpbbrklOjs74/rrr4/h4eEjXnPffffF1VdfHUuWLIndu3dPOHvqqadi8eLF1RqXClS64zfffDM6Ozujq6srvv/978d7771X7dE5BqVSKVasWBGLFy+Orq6u2L9//4Tz/v7+uOqqq+Kaa66J559/PiIihoeH47rrrovOzs744Q9/GB999NFUjM5nUMmeDxw4EN/73veiq6srrr322njnnXemYnSOUSU7/tgrr7wSl156aTXHJaOmPPLII9lvfvObLMuy7I9//GN25513Tjjfs2dP1tXVlZVKpezvf/97dtVVV5XP3njjjey73/1u1tHRUdWZ+Wwq3fF3vvOd7I033siyLMsef/zxbM2aNdUdnGPy7LPPZt3d3VmWZdlf/vKX7MYbbyyf/fOf/8yuuOKKbHR0NPv3v/9d/us777wze+KJJ7Isy7Lf/e532caNG6didD6DSvb805/+NPvTn/6UZVmWDQwMZD/4wQ+mZHaOTSU7zrIsO3DgQHbjjTdm3/jGN6Zk7lrlE+YaMzg4GPPnz4+IiAULFsRLL710xPkll1wS06ZNi7POOivGx8djeHg4Dh48GPfcc08sX758KsbmM6h0x729vXH++edHRMT4+Hg0NDRUfXaO7v/vd86cObFnz57y2e7du2Pu3LlRX18f+Xw+WlpaYu/evUf8mXjxxRenZHaOXSV77u7uLn/q6O/hE18lOx4dHY077rgjenp6pmjq2pWb6gE4frZt2xabNm2a8Nzpp58e+Xw+IiIaGxtjZGRkwnmhUIjTTjut/LixsTHef//9ciz7B/CJZbJ2PDIyErNnz46IiNdeey02b94cjz766HGenkoUCoVoamoqP66rq4vDhw9HLpeLQqFQ3n3E/+22UChMeP6T/kxw4qlkz7NmzYqIiHfeeSfWrl0bGzZsqPrcHLtKdrxq1aq47rrr4owzzpiKkWuaYD6JdXR0REdHx4Tnbr755igWixERUSwWY8aMGRPOm5qayucfv6ZQKMT+/fujp6cnRkdH4+23347Vq1fH7bfffvwvgqTJ2vHH/2B+5pln4oEHHogHH3yw/C9fTiz/vb9SqRS5XO4Tzz7e7cfPn3LKKZ/4Z4ITTyV7jojYuXNnrFy5MtatWxfnnntudYfmM/msO54+fXq8+uqr8e6778aGDRvigw8+iGXLlsW9995b9dlrkVsyakx7e3ts3749IiIGBgZi3rx5R5y/8MILUSqV4sCBA1EqlaKtrS2efvrp6Ovri97e3jjvvPPE8gmskh3PmjUrnnzyydi8eXP09fXF2WefPRWjcwza29tjYGAgIiJ27doVra2t5bO2trYYHByM0dHRGBkZiX379kVra+tR/0xw4qlkzzt37ozVq1fHQw89FBdccMFUjc4x+qw7bmtri2effTb6+vqir68vvvzlL4vlKpqWZVk21UNQPR999FF0d3fH0NBQTJ8+PdavXx/Nzc2xbt26uPzyy6OtrS1++9vfxsDAQJRKpfj5z38eF154Yfnn//a3v8WPfvSj6O/vn8KrIKWSHc+dOze+/vWvx5lnnln+9PGiiy6KW2+9dYqvhv9WKpWip6cn/vrXv0aWZbFmzZoYGBiIlpaWWLhwYfT398fWrVsjy7K44YYb4pvf/Ga899570d3dHcViMWbOnBnr16+PU089daovhYRK9vztb387xsbGorm5OSIizjnnnFi1atUUXwmfppId/38XX3xx7NixY4qmrz2CGQAAEtySAQAACYIZAAASBDMAACQIZgAASBDMAACQIJgBACBBMAMAQIJgBgCAhP8F7Mdw0f2F3GwAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualising the number of missing values by column\n",
    "sns.set(rc={'figure.figsize':(12,6)})\n",
    "plt.plot()\n",
    "sns.barplot(x = df.isnull().sum().index, y = df.isnull().sum())\n",
    "plt.xticks(rotation=60, ha = 'right')\n",
    "plt.ylabel('number of missing values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the number of CCJs, and long term debt are the 2 columns with the most missing values, though there are 6 in total with more missing values than the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualising the imbalance in the class labels\n",
    "df['y'].value_counts()\n",
    "df['y'].value_counts().plot.pie()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear class imbalance which will also need addressing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Cleaning/Munging\n",
    "\n",
    "We will then need to address issues around dimensionality (dimensionality/feature reduction, feature engineering), dealing with categorical variables in the data (currently non-numeric), class imbalance, and missing values.\n",
    "\n",
    "The above analysis shows the potential features show correlation. Although this may suggest we could benefit from reducing the dimensionality of the data, there are a few additional considerations. Firstly, in only a few cases is the correlation relationship linear.  This means that monotonically correlated features have the potential to be independently valuable at different points in the decision space. Secondly, approaches such as Principal Component Analysis would not be helpful, as they lack interpretability/intuitive decomposition of features in relation to predictions, which is a key requirement of the use case. Forward/stepwise feature selection, or recursive feature elimination could be chosen as an alternative, but subjectively, the number of features is not so large as to require it.\n",
    "\n",
    "The above analysis demonstrates that 'number_ccjs' and 'incorporation_date' are not suitable for features. H Yin et al 2020, explain the importance of domain knowledge. Ch_id is just a company identifier so this is not a useful predictor. Support for pension costs is also limited, and domain knowledge tells me this variable should not be considered explanatory either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First we drop unneeded columns\n",
    "df = df.drop(columns = ['number_ccjs', 'incorporation_date', 'pension_costs', 'ch_id'])\n",
    "\n",
    "#Now we need to map sic_division categories to numbers\n",
    "#Sklearn's label encoder provides an easy way to carry this out\n",
    "le = LabelEncoder()\n",
    "df['sic_division'] = le.fit_transform(df['sic_division'])\n",
    "\n",
    "# nb, should we wish to restore these back to their original labels, we can use the code below:\n",
    "#df['sic_division'] = le.inverse_transform(df['sic_division'])\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have several choices for handling missing values (J. Ludbroock, 2008). We could remove them, but some of the columns do have a significant number of missing values – removing them would reduce the data size considerably. An alternative could be to do mean imputation, however our analysis above shows a significant distribution in the values – the mean is unlikely to be representative and would introduce greater error.\n",
    "\n",
    "The best approach, is to use interpolation. There is no clear interpolation method that looks preferable, so I will stick with linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolating, using turnover for ordering to improve output(only column with no NA values)\n",
    "df_filled = df.sort_values(by = ['turnover']).interpolate(method = 'linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A true insolvency, where 'y' is 1, is the class of interest. This constitutes the minority of samples, so there may be limitations to a model’s ability to converge to an optimal solution. There are various ways to approach this, including ignoring it, adjusting class weighting of the cost function, random oversampling (ROS), or various Synthetic Minority Oversampling Technique (SMOTE) algorithms (F Last et al 2018). ROS, SMOTE, Adaptive Synthetic oversampling (ADASYN) etc can be carried out using the imbalanced learn library.\n",
    "\n",
    "Given the size of the dataset as whole, and the still reasonable number of minority samples (1969), the approach I have chosen is to adjust class weighting of the classifier where applicable.\n",
    "\n",
    "The final, interpolated data can be seen below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final df\n",
    "df_filled.head(5)\n",
    "#Na values are all filled\n",
    "df_filled.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model design and Development:\n",
    "### a) Preparation\n",
    "We need to chose our classifier, and prepare the data for use with the model. This also includes splitting the data into training and evaluation samples.\n",
    "\n",
    "#### 1. Chosing Classifier\n",
    "There are numerous machine algorithms available for classification. Selection of the models that best fit this usecase, is dependent on the pros and cons of these models, and how they match the business objective.\n",
    "\n",
    "* **Logistic Regression** models the probability of an observation being in a specific class, by taking the log-odds for a classification as a linear combination of independent variables, and converting the log-odds to a probability via a logit function. A threshold probability is used for classification, typically 0.5. This model is very fast to train, and interpretable, and works well with simpler relationships (Trevor Hastie et al 2009).\n",
    "\n",
    "* The **k-nearest neighbours** classifier works by assigning a class to an observation based on the class of the majority of its 'k' nearest neighbouring data points in the feature space, with k being a hyper-parameter which can be optimised. 'Nearest' in this context can be defined several ways, with Euclidean distance, Manhattan distance and Hamming distance being common methods. It is more robust to more complex relationships, and although authors such as C. Huyen suggest the results are not obviously interpretable, I would critique this, and suggest that, from the perspective of explaining how the method works to stakeholders, it is still intuitive.\n",
    "\n",
    "* A **Decision Tree** works by recursively partitioning a feature space along one feature at a time, each time splitting to maximise the information gain between the class labels. The split can be based on several approaches including Entropy and Gini information gain. It is fairly quick, and very easy to interpret, but may be prone to bias error if the tree grows too large.\n",
    "\n",
    "* A **Random Forest** is a collection of decision trees, all created with bootstrapped data. A classification is then made by voting across the various trees. It is very accurate, and the least prone to over-fitting. However, it can be slow, and the results are hard to interpret. (C. Huyen, 2022)\n",
    "\n",
    "* **Naïve Bayes** is a collection of algorithms that classifies based on Bayes’ theorem of conditional probability, assuming conditional independence between feature pairs. It is very fast to train and easily interpretable, and works well with text classification. It has comparatively lower accuracy in other types of data sets vs other classifiers (M Fernández-Delgado et al, 2015).\n",
    "\n",
    "* A **Support Vector Machine** seeks to find a single hyperplane that best splits the feature space by class label, by maximising the width of the hyperplane and thus the space between classes. It can become slow to fit with numerous features, and is highly susceptible to choice of kernel (M Fernández-Delgado et al, 2015).\n",
    "\n",
    "Based on the above evaluations, Logistic Regression, k-Nearest Neighbours and Decision Tree would be the most appropriate models for my requirements. Naïve Bayes is unlikely to be the most effective, but given its speed to train, and interpretability, I will test this too. \n",
    "\n",
    "Because my data set is not so large as to make multiple model testing prohibatively long/computationally demanding, it is appropriate to try all these different models, to see which one performs best. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Categorical Variables\n",
    "P Rodrıguez et al 2018 recommends keeping ordinal values as they are, though they may need scaling to more accurately reflect the relationship between categories. In the case of nominal categories, further treatment is needed: the most common method is one hot encoding, though there are alternative methods such as Target Encoding. Though not as intuitive, it has the advantage of avoiding sparsity, and its subsequent memory constraints (P Rodrıguez et al 2018). My own reflection is, given the size of my data, the memory available on my local machine, and that there is only 1 categorical column, sparsity is unlikely to be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_filled' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-bc8e7c3ef028>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[0mcategorical_columns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'sic_division'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m \u001b[0mdf_filled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_filled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategorical_columns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;31m#Split into feature and outcome variables, saving columns names for later\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df_filled' is not defined"
     ]
    }
   ],
   "source": [
    "#Categorical columns, even after being mapped to numbers, cannot always be left as they are, as most classifiers\n",
    "#will interpret these as having an ordinal relationship. Since the catagorical columns in this data are nominal\n",
    "# we need to One Hot Encode the catagorical variables ie sic_division\n",
    "#I am using a custom function I wrote, which can be applied to more columns, for future dataframes\n",
    "\n",
    "def one_hot_encode(df,categorical_columns):\n",
    "    for column in categorical_columns:\n",
    "        tempdf = pd.get_dummies(df[column], prefix=column)\n",
    "        df = pd.merge(\n",
    "            left=df,\n",
    "            right=tempdf,\n",
    "            left_index=True,\n",
    "            right_index=True,\n",
    "            )\n",
    "        df = df.drop(columns=column)\n",
    "        return df\n",
    "\n",
    "categorical_columns = ['sic_division']\n",
    "df_filled = one_hot_encode(df_filled, categorical_columns)\n",
    "\n",
    "#Split into feature and outcome variables, saving columns names for later\n",
    "X = df_filled.drop(columns=['y'])\n",
    "y = df_filled['y']\n",
    "X_cols = X.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model Validation Approach\n",
    "There are several options for splitting the data for evaluation including the hold-out method, also known as train-test-split. This is my preference, as it is simple to use and interpret, and my dataset is sufficiently large. It also works well for training multiple models.\n",
    "\n",
    "Although cross-validation over various k-folds can further improve the robustness of the model, Bates et al, 2022, suggest that this method may be flawed, and that using cross validated bootstrap samples of only the training data, produces a more robust model, which minimises variance error, and avoids ‘contamination’ of the test sample, improving validity of the results. This argument seems intuitively rational, and aligns well with business objectives. I will thus use k-folds cross validation for tuning hyperparemeters using only the training set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-4e2f51b226d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Now we Split data into train and test sets - using the Hold Out method. I set the training sample to 75%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#Scaling features because some classifiers like KNN require it,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#and our above analysis shows the scales for different columns vary significantly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#Now we Split data into train and test sets - using the Hold Out method. I set the training sample to 75%\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "\n",
    "#Scaling features because some classifiers like KNN require it, \n",
    "#and our above analysis shows the scales for different columns vary significantly\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Deployment\n",
    "We now chose our hyperparemeters for each respective model, and train them. Hyperparameter choices in machine learning algorithms can significantly impact performance. There are multiple options available to find the optimal set of hyperparameters.\n",
    "\n",
    "* **Grid search** – This takes a list of input values for each hyperparameter, and then tests the model on every combination of these. It can be automated in scikit learn. \n",
    "\n",
    "* An alternative is searching a grid, as above, but creating a table of the results for each hyperparameter combination, then plotting them. This can allow for a better visualisation of how the parameters interact, and can avoid 'overfitting' by finding the results which are the most robust to changes, rather than just the global max/min. However, such an approach gets hard to visualise in more than 3 dimensions. It is also worth noting, that the overfitting element can also be protected against by using kfolds validation - so the benefit is often not worth the effort/complexity. Also, in the real world, complexity and time resource must be considered when planning a model project's life cycle, and a grid search is very time/resource intensive __if__ there are many hyperparameters.\n",
    "\n",
    "* **Random search** – This takes a grid, such as above, but searches it randomly until some stopping criteria is applied. Despite its seemingly random nature, it has been shown to be surprisingly effective.\n",
    "\n",
    "* **Halvening search** – This method searches over the specified parameter values using the above methods, but with successive halving. Thus, it starts by evaluating the whole hyperparameter space with a small amount of resources, and successively selects the best candidates, using more and more resources (scikit-learn).\n",
    "\n",
    "* There also exist newer methods involving ‘bandit’ style bootstrapping of samples (**hyperopt**), and Bayesian probability to determine where next in the hyperparameter space to search (**Bayesian Optimisation**), as well as combinations of the above, such as Bayesian Optimised Hyperband (**BOHB**).\n",
    "\n",
    "\n",
    "(B. Bischl et al 2021)\n",
    "\n",
    "Given the fact we are using classifiers with comparatively few high impact hyperparameters, we opt to use a grid search in this instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiate instance of various classifiers, making the weighting balanced where needed, and using max CPU cores for speed\n",
    "DT = DecisionTreeClassifier(class_weight = 'balanced')\n",
    "NB = GaussianNB()\n",
    "KNN = KNeighborsClassifier(n_jobs = -1)\n",
    "LR = LogisticRegression(class_weight = 'balanced', n_jobs = -1, max_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-50ccc0727c6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                       \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                       cv=5)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgs_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mgs_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgs_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Decision Tree\n",
    "dt_params = [{'max_depth': [5, 10, 15], #nb further depth will liely improve the classifier, but increases risk of overfit and\n",
    "         'min_samples_leaf': [1,3,5], #loses interpretability\n",
    "         'splitter': ['best','random']}]\n",
    "gs_dt = GridSearchCV(DT,\n",
    "                      param_grid=dt_params,\n",
    "                      scoring='f1_weighted',\n",
    "                      cv=5)\n",
    "gs_dt.fit(X_train, y_train)\n",
    "gs_dt.best_params_\n",
    "gs_dt.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-ff71c5b76e84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                       \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                       cv=5)\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgs_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mgs_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgs_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Naive Bayes\n",
    "nb_params = [{'var_smoothing': [0.000000001, 0.0000000001]}]\n",
    "gs_nb = GridSearchCV(NB,\n",
    "                      param_grid=nb_params,\n",
    "                      scoring='f1_weighted',\n",
    "                      cv=5)\n",
    "gs_nb.fit(X_train, y_train)\n",
    "gs_nb.best_params_\n",
    "gs_nb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-d62e01741eed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m                       \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m                       cv=5)\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mgs_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mgs_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mgs_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#KNN\n",
    "knn_params = [{'n_neighbors': range(3,20),#[4, 5, 6, 7, 8, 9,10, 11, 12, 13, 14, 15],\n",
    "         'weights': ['uniform', 'distance'],\n",
    "         'leaf_size': [15, 30, 45]}]\n",
    "gs_knn = GridSearchCV(KNN,\n",
    "                      param_grid=knn_params,\n",
    "                      scoring='f1_weighted',\n",
    "                      cv=5)\n",
    "gs_knn.fit(X_train, y_train)\n",
    "gs_knn.best_params_\n",
    "gs_knn.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-46c7e1b00c30>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m                       \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'f1_weighted'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                       cv=5)\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgs_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mgs_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mgs_lr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "#Logistic Regression\n",
    "lr_params = [{'C': [0.3, 0.5, 0.7, 1,]}]\n",
    "gs_lr = GridSearchCV(LR,\n",
    "                      param_grid=lr_params,\n",
    "                      scoring='f1_weighted',\n",
    "                      cv=5)\n",
    "gs_lr.fit(X_train, y_train)\n",
    "gs_lr.best_params_\n",
    "gs_lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation:\n",
    "### a) Examples of Predictions\n",
    "We produce a few examples of the models' predictions on the test data, compared against the actual label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ebe498503261>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# EXAMPLES from the different classifiers, Predicting examples from the test sample when compared to the real label\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtest_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_actual'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_dt_pred'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'y_nb_pred'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_nb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m27\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# EXAMPLES from the different classifiers, Predicting examples from the test sample when compared to the real label\n",
    "test_df = pd.DataFrame(X_test, columns = X_cols)\n",
    "test_df['y_actual']= y_test.values\n",
    "test_df['y_dt_pred'] = gs_dt.predict(test_df.iloc[:, 0:27])\n",
    "test_df['y_nb_pred'] = gs_nb.predict(test_df.iloc[:, 0:27])\n",
    "test_df['y_knn_pred'] = gs_knn.predict(test_df.iloc[:, 0:27])\n",
    "test_df['y_lr_pred'] = gs_lr.predict(test_df.iloc[:, 0:27])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Results: Model Evaluation Scores and Confusion Matrices\n",
    "Given the business requirements, the most appropriate overall evaluation metric is the F1 score, which is the harmonic mean of the precision and the recall. The precision alone is also important to consider, as this is the first business goal. We loop through each classifier, and evaluate the results of the model on the out-of-sample test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-58f5cf69a5bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "#Test results of each, using various metrics\n",
    "model_list = [('Decision Tree', gs_dt), ('Naive Bayes', gs_nb), ('K-Nearest Neighbors', gs_knn), ('Logistic Regression', gs_lr)]\n",
    "\n",
    "names = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "f1_weighteds = []\n",
    "tns = []\n",
    "fps = []\n",
    "fns = []\n",
    "tps = []\n",
    "\n",
    "for name, model in model_list:\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred, normalize = 'pred').ravel() * 100\n",
    "    \n",
    "    names.append(name)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)\n",
    "    f1_weighteds.append(f1_weighted)\n",
    "    tns.append(tn)\n",
    "    fps.append(fp)\n",
    "    fns.append(fn)\n",
    "    tps.append(tp)\n",
    "    \n",
    "\n",
    "summary = pd.DataFrame({'Name':names,\n",
    "                       'Precision':precisions,\n",
    "                       'F1': f1s,\n",
    "                       'Weighted F1':f1_weighteds,\n",
    "                       'True Positive %':tps,\n",
    "                       'True Negative %': tns,\n",
    "                       'False Positive %': fps,\n",
    "                        'False Negative %': fns})\n",
    "summary.sort_values(by = 'F1', ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results clearly show the two best models, by all the relevant evaluation metrics, are the K-Nearest Neighbour and the Decision Tree, with the KNN substantially better by all metrics. Just for clarity, I visualise the Confusion matrix for these below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConfusionMatrixDisplay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-26-fa61555a2625>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Confusion matrices for the test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Decision Tree Confusion Matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConfusionMatrixDisplay' is not defined"
     ]
    }
   ],
   "source": [
    "#Confusion matrices for the test data\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred = gs_dt.predict(X_test))\n",
    "plt.title('Decision Tree Confusion Matrix')\n",
    "plt.show()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred = gs_knn.predict(X_test))\n",
    "plt.title('KNN Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ConfusionMatrixDisplay' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-2487cc0d086b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Confusion matrices for the test data, normalised to account for imbalance\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_dt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pred'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Decision Tree Confusion Matrix- Normalised by prediction'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mConfusionMatrixDisplay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_predictions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnormalize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'pred'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ConfusionMatrixDisplay' is not defined"
     ]
    }
   ],
   "source": [
    "#Confusion matrices for the test data, normalised to account for imbalance\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred = gs_dt.predict(X_test), normalize = 'pred')\n",
    "plt.title('Decision Tree Confusion Matrix- Normalised by prediction')\n",
    "plt.show()\n",
    "ConfusionMatrixDisplay.from_predictions(y_test, y_pred = gs_knn.predict(X_test), normalize = 'pred')\n",
    "plt.title('KNN Confusion Matrix- Normalised by prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Evaluation Observations:\n",
    "\n",
    "* We have 9386 records (25% of the original data set)\n",
    "* In the un-normalised version, For Decision Tree and KNN, we can see the square in the top left is bright yellow, showing a high number of negative outcomes being properly identified. Given the imbalance in the data, this is to be expected\n",
    "* The other squares are dark , showing the number of false negatives, true positives, and false positves, are all quite low. Again, in unbalanced data, this is to be expected\n",
    "* In the normalised version, we can see the Decision Tree has a blue colour in the lower right box now. This means the percentage of firms that become insolvent, that are corectly identified as such, is higher now, at 25%, but the number of false positives is still very high, 75% - objectively this is still not very good.\n",
    "* The KNN is much better for reducing false positives, only 14% of firms which were fine, were predicted to become insolvent. \n",
    "\n",
    "__Based on the above, the KNN would definately be the model of choice for live deployment__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) A point on inspecting robustness of results for KNN\n",
    "We can see how the metric (Weighted F1) changes as the hyperparameters change. This is best represented as heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-ffa3076595bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Get the grid search results into a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mknn_param_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_knn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mk_uniform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_param_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mknn_param_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'param_weights'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'uniform'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#split into 2 heatmaps\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mk_distance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mknn_param_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mknn_param_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'param_weights'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'distance'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#pivot the data into a format for the heatmap, and plot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "#Get the grid search results into a dataframe\n",
    "knn_param_df = pd.DataFrame(gs_knn.cv_results_)\n",
    "k_uniform = knn_param_df[knn_param_df['param_weights'] == 'uniform'] #split into 2 heatmaps\n",
    "k_distance = knn_param_df[knn_param_df['param_weights'] == 'distance']\n",
    "#pivot the data into a format for the heatmap, and plot\n",
    "sns.heatmap(pd.pivot(k_uniform, index='param_leaf_size', columns='param_n_neighbors', values='mean_test_score'), cmap = 'PiYG')\n",
    "plt.title('Weights is \"Uniform\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'k_distance' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-6b16167e402a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheatmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpivot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk_distance\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'param_leaf_size'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'param_n_neighbors'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'mean_test_score'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'PiYG'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Weights is \"Distance\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'k_distance' is not defined"
     ]
    }
   ],
   "source": [
    "sns.heatmap(pd.pivot(k_distance, index='param_leaf_size', columns='param_n_neighbors', values='mean_test_score'), cmap = 'PiYG')\n",
    "plt.title('Weights is \"Distance\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above, that leaf size has little to no effect on the final result. The optimal k number of 13 is quite robust, in that small adjustments in the optimal number don’t significantly alter the results. This lends support to the assertion that the model is not over-fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) Final Exercise on Model Visualisation\n",
    "Whilst the KNN is intuitively easy to understand, it is harder to breakdown the exact variables which have led to a single decision. We may also wish to consider the second best performing model, in case the interpretability of this makes it more beneficial for the client, even at the expense of evaluation accuracy. Thus, just for reference, I have included a visualisation of the Decision Tree Model\n",
    "\n",
    "A few things to note:\n",
    "* A gridsearched object cannot be plotted to a tree, so we have to make a new decision tree classifier object\n",
    "* Our optimised tree still had a very high depth and was not cost complexity pruned. The below example shows that despite the flow diagram, the tree is not as easy to interpret as it intially appears \n",
    "* Decision trees work on information gain, so scaling is unnecessary. Being able to use the original inputs adds interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-d408eced1856>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Train Test split without scaling\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#Fit Decision tree classifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplitter\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'random'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#Train Test split without scaling\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n",
    "#Fit Decision tree classifier\n",
    "clf = DecisionTreeClassifier(class_weight = 'balanced', max_depth= 15, min_samples_leaf= 1, splitter= 'random')\n",
    "clf.fit(X_train, y_train)\n",
    "#plot resulting tree (tree is very large, needs a large image size to display)\n",
    "fig = plt.figure(figsize = (200,75))\n",
    "_ = tree.plot_tree(clf, \n",
    "                   feature_names=X_cols,  \n",
    "                   class_names= ['solv','insolv'],\n",
    "                   filled=True,\n",
    "                  fontsize=6)\n",
    "plt.show()\n",
    "#If you wish to save image\n",
    "#fig.savefig(filepath, dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "We have shown the data science lifecycle of this project, through data discovery and analysis. Different modelling choices have been trained and evaluated. The final model we should use, in my opinion, is the KNN, based on its superior evaluation metrics. This trained model can be captured for use by the client using either the pickle or joblib libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflection\n",
    "Overall, the project has been successful. The KNN model produced good evaluation metrics, and fulfilled the brief in terms of the requirements for explainability. If the KNN was not considered explainable enough, the decision tree could be used as an alternative, though it comes with poorer evaluation outcomes. There were, however, some things that might have been improved upon in the project\n",
    "\n",
    "For the interpolation of the missing values in the data, we used a linear method. Given the irregular shape of the variable distributions, the linear method might not be the ideal choice. One option would be to create line graphs for the various columns, arranged in ascending order, to see what pattern they take. We could then apply the appropriate method: linear, quadratic, cubic etc. If the line takes no clear pattern, then a spline may be better suited.\n",
    "\n",
    "In regards to the timing of the interpolation, there was another consideration to factor. Ideally, the interpolation should be done after splitting the data into train/test sets. If any future data used for predictions is missing values, the imputation with a full data set would be different to imputation on just the sample. So imputation of the whole dataset in advance taints the testing sample. This is the same reason the scaler transforms the train and test samples, but is only fit on the training data.\n",
    "\n",
    "One key stage in the data cleaning process that was not addressed, was the removal of outliers. In the case of this notebook, this was due to business requirements, but in most cases, there would usually be a desire to remove extreme and spurious data points which would reduce the goodness of fit of the classifier. We could use the scatter matrix above to try to visualise these, as well as do histograms/violin plots for each feature, so look for any unusual shapes in the distributions. We can remove these by either doing a general trim of those points furthest from the mean, or applying a winsorisation instead. \n",
    "One method to do this process computationally, is to use sklearn.covariance.EllipticEnvelope (Rousseeuw, P.J., Van Driessen, K). This models fits the initial data, and then creates a binary label for each data point, describing it as an outlier or not. We can then filter to exclude the outliers. \n",
    "\n",
    "Another area we could have looked at was alternative measurements of classifier fit, such as AUCROC. This measures the goodness of fit across different probability predictions (sklearn’s default is to use a 50% probability threshold). This is an interesting metric, as it can sometimes reveal overfitting due to probability threshold selection. Indeed, the probability threshold could also have been tested as a hyperparameter, using the predict_proba() method.\n",
    "\n",
    "The sample splitting for the project was done using train test split, with hyperparameter evaluation done using kfolds cross validation. There were good reasons for this, as stated above, but I may have dismissed a stratified k-folds cross validation approach too readily. Keeping a hold out test set was recommended by Bates et al, 2022, and the scikit-learn documentation, but given the imbalanced nature of the data, a stratified fold may have improved the fit of the classifier.  My objection on this mainly came in the form of wanting to avoid overfitting, since a stratified sample is not representative of the natural variance found in new data.\n",
    "\n",
    "One other point to reflect on, was the treatment of the Naïve Bayes model. This model doesn’t fundamentaly handle a mix of categorical and discrete data features.  I have used the Gaussian model given the nature of most of the features, but in reality, this will have poor results for the binary features. The ideal approach here would have been to use two models: one to find the probability from the categorical variables (using predict_proba from sklearn's BernoulliNB model, fit to the sector variable), and another to calculate the probabilities from the other continuous variables (using predict_proba from sklearn's GaussianNB). We then have to multiply the probabilities from both models and divide by the prior from either model, and then divide again by the sum (over the classes) to normalize. In reality, given the poor probability of the NB model, it is unlikely to have impacted the final model choice.\n",
    "\n",
    "Lastly, further to the model options discussed above, there are also relatively newer options available, including gradient boosting methods (eg XGBoost, lightGBM) that sequentially build from weak learners, and also neural networks. These could inform updates on the project in the future, though they still lack interpretability. Much as I mention above, how the KNN may be rejected due to lack of interpretability, it may go the other way. In some cases, stakeholders may opt to change their requirements in the future, dialling down the desire for interpretability in exchange for significantly better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. https://www.investopedia.com/terms/i/interestcoverageratio.asp accessed 2023\n",
    "2. The Importance of Domain Knowledge, Haixing Yin, Fan Fan, Jiazhi Zhang, Hanyang Li and Ting Fung Lau, MLD, CMU, August 31, 2020\n",
    "3. Exploratory Data Analysis, John W. Tukey, 1977\n",
    "4. Ludbrook J. Outlying observations and missing values: how should they be handled? Clin Exp Pharmacol Physiol. 2008;35(5–6):670–8.\n",
    "5. Felix Last, Georgios Douzas, Fernando Bacao, Oversampling for Imbalanced Learning Based on K-Means and SMOTE, Information Sciences 465 (2018) 1-20\n",
    "6. Hyperparameter Optimization: Foundations, Algorithms, Best Practices and Open Challenges Bernd Bischl, Martin Binder , Michel Lang, Tobias Pielok , Jakob Richter , Stefan Coors , Janek Thomas , Theresa Ullmann , Marc Becker , Anne-Laure Boulesteix , Difan Deng , and Marius Lindauer, Department of Statistics, Ludwig-Maximilians-Universit¨at M¨unchen – 24 Nov 2021\n",
    "7. https://scikit-learn.org/ accessed 2023\n",
    "8. Trevor Hastie, Robert Tibshirani, and Jerome Friedman, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, 2009\n",
    "9. Chip Huyen, Designing Machine Learning Systems: An Iterative Process for Production-Ready Applications – 31 May 2022\n",
    "10. Manuel Fernández-Delgado, Eva Cernadas, Senén Barro, Dinani Amorim, Do we need hundreds of classifiers to solve real world classification problems?. The Journal of Machine Learning Research, Volume 15, Issue 1January 2014 pp 3133–3181\n",
    "11. Beyond One-hot Encoding: lower dimensional target embedding Pau Rodrıguez, Miguel A. Bautista, Jordi Gonzalez, Sergio Escalera, Computer Vision Center, Universitat Autonoma de Barcelona, Spain, Heidelberg Collaboratory for Image Processing, Heidelberg University, Germany, University of Barcelona, Barcelona, Spain, May 17, 2018\n",
    "12. Stephen Bates, Trevor Hastie, Robert Tibshirani, Cross-validation: what does it estimate and how well does it do it? 2022\n",
    "13. Rousseeuw, P.J., Van Driessen, K. “A fast algorithm for the minimum covariance determinant estimator” Technometrics 41(3), 212 (1999)\n",
    "14. https://scikit-learn.org/stable/modules/cross_validation.html accessed 2023\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
